{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport subprocess\n\nimport pandas as pd\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision.models import resnet18\nfrom torch.utils.data import DataLoader, Dataset\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n\n#%conda install -c conda-forge opacus\n!git clone https://github.com/pytorch/opacus.git\n%cd opacus\n%pip install -e .\nimport opacus","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-24T09:46:24.626314Z","iopub.execute_input":"2023-09-24T09:46:24.627432Z","iopub.status.idle":"2023-09-24T09:46:43.526427Z","shell.execute_reply.started":"2023-09-24T09:46:24.627397Z","shell.execute_reply":"2023-09-24T09:46:43.525327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# It's really important to add an accelerator to your notebook, as otherwise the submission will fail.\n# We recomment using the P100 GPU rather than T4 as it's faster and will increase the chances of passing the time cut-off threshold.\n\nif DEVICE != 'cuda':\n    raise RuntimeError('Make sure you have added an accelerator to your notebook; the submission will fail otherwise!')","metadata":{"execution":{"iopub.status.busy":"2023-09-22T02:18:49.976053Z","iopub.execute_input":"2023-09-22T02:18:49.976754Z","iopub.status.idle":"2023-09-22T02:18:49.98739Z","shell.execute_reply.started":"2023-09-22T02:18:49.976724Z","shell.execute_reply":"2023-09-22T02:18:49.984079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Helper functions for loading the hidden dataset.\n\ndef load_example(df_row):\n    image = torchvision.io.read_image(df_row['image_path'])\n    result = {\n        'image': image,\n        'image_id': df_row['image_id'],\n        'age_group': df_row['age_group'],\n        'age': df_row['age'],\n        'person_id': df_row['person_id']\n    }\n    return result\n\n\nclass HiddenDataset(Dataset):\n    '''The hidden dataset.'''\n    def __init__(self, split='train'):\n        super().__init__()\n        self.examples = []\n\n        df = pd.read_csv(f'/kaggle/input/neurips-2023-machine-unlearning/{split}.csv')\n        df['image_path'] = df['image_id'].apply(\n            lambda x: os.path.join('/kaggle/input/neurips-2023-machine-unlearning/', 'images', x.split('-')[0], x.split('-')[1] + '.png'))\n        df = df.sort_values(by='image_path')\n        df.apply(lambda row: self.examples.append(load_example(row)), axis=1)\n        if len(self.examples) == 0:\n            raise ValueError('No examples.')\n\n    def __len__(self):\n        return len(self.examples)\n\n    def __getitem__(self, idx):\n        example = self.examples[idx]\n        image = example['image']\n        image = image.to(torch.float32)\n        example['image'] = image\n        return example\n\n\ndef get_dataset(batch_size):\n    '''Get the dataset.'''\n    retain_ds = HiddenDataset(split='retain')\n    forget_ds = HiddenDataset(split='forget')\n    val_ds = HiddenDataset(split='validation')\n\n    retain_loader = DataLoader(retain_ds, batch_size=batch_size, shuffle=True)\n    forget_loader = DataLoader(forget_ds, batch_size=batch_size, shuffle=True)\n    validation_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=True)\n\n    return retain_loader, forget_loader, validation_loader","metadata":{"execution":{"iopub.status.busy":"2023-09-22T02:18:49.98962Z","iopub.execute_input":"2023-09-22T02:18:49.989993Z","iopub.status.idle":"2023-09-22T02:18:50.008128Z","shell.execute_reply.started":"2023-09-22T02:18:49.989961Z","shell.execute_reply":"2023-09-22T02:18:50.007156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# You can replace the below simple unlearning with your own unlearning function.\n\nfrom opacus import PrivacyEngine\n\ndef unlearning(\n    net, \n    retain_loader, \n    forget_loader, \n    val_loader):\n    print(\"\"\"Simple unlearning by finetuning.\"\"\")\n    epochs = 1\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(net.parameters(), lr=0.0013,\n                      momentum=0.9, weight_decay=5e-4)\n#     optimizer = optim.Adam(net.parameters(), lr=0.001)\n\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n    \n    # enter PrivacyEngine: 使用pytorch已有的DP-SGD privacy_engine\n    privacy_engine = PrivacyEngine()\n    model, optimizer, data_loader = privacy_engine.make_private(\n        module=net,\n        optimizer=optimizer,\n        data_loader=forget_loader,\n        noise_multiplier=1.1,\n        max_grad_norm=1.0,\n    )\n    \n    #learning rate adjustment:\n    '''\n    Increase the initial learning rate. Because the learning rate will very likely decrease, start with a larger value to decrease from. A larger learning rate will result in a lot larger changes to the weights, at least in the beginning, allowing you to benefit from the fine-tuning later.\n    Use a large momentum. Many optimizers can consider momentum. Using a larger momentum value will help the optimization algorithm continue to make updates in the right direction when your learning rate shrinks to small values.\n    Experiment with different schedules. It will not be clear which learning rate schedule to use, so try a few with different configuration options and see what works best on your problem. Also, try schedules that change exponentially and even schedules that respond to the accuracy of your model on the training or test datasets.\n    '''\n    \n    net.train()\n\n    for ep in range(epochs):\n        net.train()\n        for sample in retain_loader:\n            inputs = sample[\"image\"]\n            targets = sample[\"age_group\"]\n            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n            optimizer.zero_grad()#先将梯度归零\n            outputs = net(inputs)\n            loss = criterion(outputs, targets)\n            loss.backward()#反向传播计算得到每个参数的梯度值\n            optimizer.step()#参数更新\n        scheduler.step()\n        \n    net.eval()","metadata":{"execution":{"iopub.status.busy":"2023-09-24T09:47:48.580556Z","iopub.execute_input":"2023-09-24T09:47:48.580979Z","iopub.status.idle":"2023-09-24T09:47:48.594080Z","shell.execute_reply.started":"2023-09-24T09:47:48.580924Z","shell.execute_reply":"2023-09-24T09:47:48.593186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nif os.path.exists('/kaggle/input/neurips-2023-machine-unlearning/empty.txt'):\n    # mock submission\n    subprocess.run('touch submission.zip', shell=True)\nelse:\n    \n    # Note: it's really important to create the unlearned checkpoints outside of the working directory \n    # as otherwise this notebook may fail due to running out of disk space.\n    # The below code saves them in /kaggle/tmp to avoid that issue.\n    \n    os.makedirs('/kaggle/tmp', exist_ok=True)\n    retain_loader, forget_loader, validation_loader = get_dataset(64)\n    \n    net = resnet18(weights=None, num_classes=10)\n    net.to(DEVICE)\n    for i in range(512):\n        net.load_state_dict(torch.load('/kaggle/input/neurips-2023-machine-unlearning/original_model.pth'))\n        unlearning(net, retain_loader, forget_loader, validation_loader)\n        state = net.state_dict()\n        torch.save(state, f'/kaggle/tmp/unlearned_checkpoint_{i}.pth')\n        \n    # Ensure that submission.zip will contain exactly 512 checkpoints \n    # (if this is not the case, an exception will be thrown).\n    unlearned_ckpts = os.listdir('/kaggle/tmp')\n    if len(unlearned_ckpts) != 512:\n        raise RuntimeError('Expected exactly 512 checkpoints. The submission will throw an exception otherwise.')\n        \n    subprocess.run('zip submission.zip /kaggle/tmp/*.pth', shell=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-22T02:18:50.029699Z","iopub.execute_input":"2023-09-22T02:18:50.03028Z","iopub.status.idle":"2023-09-22T02:18:50.053847Z","shell.execute_reply.started":"2023-09-22T02:18:50.030245Z","shell.execute_reply":"2023-09-22T02:18:50.052825Z"},"trusted":true},"execution_count":null,"outputs":[]}]}